---
title: GWU TwitterSets & the tweet Hydrator
author: ''
date: '2020-07-15'
slug: gwu-twittersets-the-tweet-hydrator
categories: []
tags: []
---



<div id="quick-thoughts-historical-twitter-corpus" class="section level2">
<h2>Quick thoughts historical Twitter corpus</h2>
<p>Some notes and reminders re:Twitter. I use the R package <code>rtweet</code> for accessing tweets. It is fantastic. I generally do not get greedy when it comes to Twitter data; I work with well-defined universes/research questions and focus on methods. That said, I recently became interested in building a Twitter corpus of all US lawmaker tweets from congresses 115 &amp; 116. So, 3 January 2017 to present day.</p>
<p>Per my limited understanding, if I wanted to build this corpus today, I would run into (at least) two problems: (1) I would be limited to querying only the last 3,200 statuses posted or retweeted per lawmaker, and (2) I would not have access to accounts/tweets that have been deleted. And folks that leave office often abandon (ie, delete) official congressional accounts.</p>
<p>The only real way to build an historical Twitter corpus, then, is to build one in real time, ie, scrape tweets for some universe of tweeters weekly or monthly, etc. For me, that ship has sailed. But other/smarter folks have been doing this – eg – the <a href="https://tweetsets.library.gwu.edu/">George Washington University (GWU) Library</a>.</p>
<p>However, Twitter does not allow GWU to share the actual tweet data – only the tweet ids. So, I can download the list of ids for the 2M or so tweets generated by the members of the 115th US Congress, and then obtain the actual tweet data via a tweet hydrator. DocNow’s <a href="https://github.com/DocNow/hydrator">Hydrator</a> is very slick. This allows me to go a bit further back in time than <code>rtweet</code> (not clear how/why); and while I still cannot obtain tweets from accounts that have been deleted, I can build a fairly functional historical corpus of tweets composed by members of US Congresses 115 &amp; 116. And moving forward, I can update it on my own.</p>
<p>A note on coverage: Of the ~1.6M (original content) tweet ids included in GWU’s US Congress 116 tweet cache, only ~1% have been deleted (as of 7.14.2020). Also note: in some cases, tweets included in GWU list for a given congress were not generated while that congress was in session.</p>
</div>
<div id="load-tweets-as-corpus" class="section level2">
<h2>Load tweets as corpus</h2>
<p>RTs excluded. Tweets limited to duration of congress. Accessed mid-July 2020.</p>
<pre class="r"><code>library(tidyverse)
setwd(tweet_dir)

gfiles &lt;- list.files(path = tweet_dir, 
                     pattern = &quot;rds&quot;, 
                     recursive = TRUE) 

corpus_raw &lt;- lapply(gfiles, readRDS) 
corpus_tif &lt;- data.table::rbindlist(corpus_raw) </code></pre>
</div>
<div id="corpus-descriptives" class="section level2">
<h2>Corpus descriptives</h2>
<p>Some important trends. Some (potential) explanations:</p>
<ol style="list-style-type: decimal">
<li><p>Average length of tweets (in words) increased substantially from 2017 to 2018; presumably a result of Twitter’s increased character limit from 140 to 280 at the end of 2017.</p></li>
<li><p>Total number of Twitter lawmaker accounts roughly doubled from the 115th congress to the 116th. Without any real effects on the number of total tweets from 115 to 116. The folks at GWU included “auxiliary” accounts for members in the 116th Twitter scrape, eg, both “office” &amp; “campaign” accounts.</p></li>
</ol>
<p>It is not clear if this is per some methodological change on the part of GWU, or a result of more lawmakers in 116 using more than one account.</p>
<pre class="r"><code>xsum1 &lt;- corpus_tif %&gt;%
  mutate(tokens = tokenizers::count_words(text),
         yr = gsub(&#39;-.*&#39;, &#39;&#39;, created_at))

xsum2 &lt;- xsum1 %&gt;%
  group_by(congress, yr) %&gt;%
  summarise(tweets = n(),
            tokens = sum(tokens),
            ers = length(unique(toupper(user_screen_name))),
            ave = round(tokens/tweets, 1)) %&gt;%
  mutate(tweets = formatC(tweets, big.mark = &#39;,&#39;),
         tokens = formatC(tokens, big.mark = &#39;,&#39;))

xsum2 %&gt;% knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">congress</th>
<th align="left">yr</th>
<th align="left">tweets</th>
<th align="left">tokens</th>
<th align="right">ers</th>
<th align="right">ave</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">115</td>
<td align="left">2017</td>
<td align="left">382,281</td>
<td align="left">8,101,282</td>
<td align="right">513</td>
<td align="right">21.2</td>
</tr>
<tr class="even">
<td align="left">115</td>
<td align="left">2018</td>
<td align="left">326,864</td>
<td align="left">10,606,772</td>
<td align="right">510</td>
<td align="right">32.5</td>
</tr>
<tr class="odd">
<td align="left">115</td>
<td align="left">2019</td>
<td align="left">507</td>
<td align="left">13,610</td>
<td align="right">267</td>
<td align="right">26.8</td>
</tr>
<tr class="even">
<td align="left">116</td>
<td align="left">2019</td>
<td align="left">354,711</td>
<td align="left">12,367,502</td>
<td align="right">894</td>
<td align="right">34.9</td>
</tr>
<tr class="odd">
<td align="left">116</td>
<td align="left">2020</td>
<td align="left">140,556</td>
<td align="left">5,025,820</td>
<td align="right">873</td>
<td align="right">35.8</td>
</tr>
</tbody>
</table>
</div>
<div id="daily-tweets" class="section level2">
<h2>Daily tweets</h2>
<p>3 Jan 2017 - 5 May 2020; With rolling bi-monthly averages in blue. Still need to include tweets from the last several months. So, despite disparities in the number of Twitter accounts associated with each congress, a fairly smooth transition from congress 115 to 116 in terms of daily tweet count; but a transition nonetheless – which seems reasonable. And some fairly intuitive congressional/yearly patterns in variation.</p>
<pre class="r"><code>xsum1 %&gt;%
  group_by(congress, yr, created_at) %&gt;%
  summarise(tweets = n(),
            tokens = sum(tokens),
            ers = length(unique(user_screen_name))) %&gt;%
  ungroup() %&gt;%
  mutate(moving_n = zoo::rollmean(tweets, 
                                  k = 60, 
                                  fill = NA)) %&gt;%
  ggplot() +
  geom_line(aes(x = created_at, 
                y = tweets)) +
  
    geom_line(aes(x = created_at,
                y = moving_n),
            color = &#39;steelblue&#39;,
            #linetype = 2,
            size = 0.75) +
  ggtitle(&#39;Daily tweets: US Congresses 115 &amp; 116&#39;)</code></pre>
<p><img src="/post/2020-08-05-gwu-twittersets-the-tweet-hydrator_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Littman, Justin, 2017, “115th U.S. Congress Tweet Ids”, <a href="https://doi.org/10.7910/DVN/UIVHQR" class="uri">https://doi.org/10.7910/DVN/UIVHQR</a>, Harvard Dataverse, V5, UNF:6:pa6q3/72341rRixB7ez15Q== [fileUNF]</p>
<p>Wrubel, Laura; Kerchner, Daniel, 2020, “116th U.S. Congress Tweet Ids”, <a href="https://doi.org/10.7910/DVN/MBOJNS" class="uri">https://doi.org/10.7910/DVN/MBOJNS</a>, Harvard Dataverse, V1, UNF:6:WKfCGiEnn3FJi7QhnswUsA== [fileUNF]</p>
</div>
